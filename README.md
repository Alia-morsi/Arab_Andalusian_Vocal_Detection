This work is an attempt at training a model to automate the annotation of vocal/non-vocal regions of Arab-Andalusian music recordings. A large part of this work was devoted to feature extraction and dataset creation. At the moment, only spectral features are used for training, namely mel-frequency ceptral coefficients (MFCCs). However, it yet remains to incorporate temporal MFCC features from derivatives and spectral flux values. Artificial Neural Networks (ANNs) are used for this task. The dataset that was created consists of 4 recordings from the Arab Andalusian Corpus on Dunya, totaling of 190 minutes approximately, and with the audio sections of each annotated as either vocal/instrumental. Evaluation scores obtained by using mir_eval evaluation framework
